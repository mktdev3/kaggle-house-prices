# 今回の処理の流れとポイント

## 1. 概要

Kaggle House Prices コンペティションに向けた、データ前処理 (`src/preprocess.py`) からモデル学習・推論 (`src/train.py`) までの一連の処理フローをまとめました。

## 2. 前処理 (`src/preprocess.py`)

データの一貫性を保つため、学習データとテストデータを一時的に結合して処理を行っています。

### データのクレンジングと欠損値補完
- **目的変数 (`SalePrice`) の変換**: 分布の歪みを補正するため、対数変換 (`np.log1p`) を適用しています。
- **`LotFrontage` (間口)**: 近隣地域 (`Neighborhood`) ごとの中央値で補完しています。
- **欠損値の意味づけ**:
    - ガレージや地下室関連の数値項目 (`GarageArea` 等) は、設備がないと仮定して `0` で補完。
    - カテゴリカル項目 (`PoolQC`, `GarageType` 等) は、設備なしを意味する `"None"` で補完。
    - その他 (`Functional`, `Electrical` 等) は最頻値 (`mode`) で補完。

### 特徴量エンジニアリング (独自変数の作成)
住宅の価値に直結しそうな新しい特徴量を作成・追加しています。
- **`TotalSF` (総面積)**: 地下面積 + 1階面積 + 2階面積
- **`HouseAge` (築年数)**: 販売年 - 建築年
- **`RemodAge` (リフォーム経過年数)**: 販売年 - リフォーム年
- **`TotalBath` (バスルーム総数)**: フルバスルーム + 0.5 × ハーフバスルーム (地下含む)
- **`TotalPorchSF` (ポーチ総面積)**: 各種ポーチ・デッキ面積の合計

### カテゴリカル変数の処理
- **Label Encoding**: 全てのカテゴリカル変数を数値ラベルに変換しています。決定木ベースのモデル（XGBoost, LightGBM）が扱いやすい形式です。

### 出力
処理済みデータは `data/processed/` フォルダに保存されます。
- `train_x.csv`: 学習用特徴量
- `train_y.csv`: 学習用目的変数
- `test_x.csv`: テスト用特徴量
- `test_id.csv`: テスト用ID (提出ファイル作成用)

## 3. モデル学習と評価 (`src/train.py`)

精度の向上と過学習のリスク低減を目指し、特性の異なる複数のモデルを組み合わせるアンサンブル手法を採用しています。

### 使用モデル
1.  **Ridge Regression (リッジ回帰)**: 線形モデル。線形な関係性を捉えるベースラインとして使用。
2.  **XGBoost**: 勾配ブースティング決定木。強力な非線形モデル。
3.  **LightGBM**: 勾配ブースティング決定木。高速かつ高精度。

### 学習・評価戦略
- **K-Fold Cross Validation (交差検証)**: データを5分割して検証を行い、モデルの汎化性能を正しく評価します (RMSEを使用)。
- **アンサンブル (Weighted Averaging)**:
    - 各モデルの予測値を加重平均して最終予測とします。
    - 比率: Ridge (0.2) + XGBoost (0.4) + LightGBM (0.4)
    - 決定木モデルに重きを置きつつ、線形モデルも混ぜることでロバスト性を高めています。

### 提出
- 最終的な予測値は対数変換からもとのスケールに戻して (`np.expm1`)、`data/submission.csv` として保存されます。
